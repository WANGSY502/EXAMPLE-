{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the basic structure of the code !\n",
    "\n",
    "- I'm giving you the structure you need to parse the critic name, but you'll have to do the rest by yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# parameters of our file\n",
    "movie   = 'parasite_2019'\n",
    "name    = 'apostolos_filippas' # replace here with your name!\n",
    "pageNum = 10\n",
    "\n",
    "# create the movie url\n",
    "url     = 'https://www.rottentomatoes.com/m/'+movie+'/reviews/'\n",
    "\n",
    "# output file\n",
    "with open('data/'+name + '_' + movie+'.txt','w') as fw:\n",
    "    \n",
    "    # for each page out of the pageNum pages you want to parse\n",
    "    for p in range(1,pageNum+1): \n",
    "        \n",
    "        # tell user which page you're parsing\n",
    "        print ('Getting page',p)\n",
    "        \n",
    "        # initialize html file to None\n",
    "        html=None        \n",
    "        \n",
    "        # set URL to get appropriately\n",
    "        #   if it is the first page\n",
    "        if p==1: \n",
    "            # url for page 1\n",
    "            pageLink=url \n",
    "        #   if it is tnot the first page\n",
    "        else: \n",
    "            # url for other pages\n",
    "            pageLink=url+'?page='+str(p)+'&sort=' # make the page url\n",
    "            \n",
    "        # try to scrape times\n",
    "        for i in range(5): \n",
    "            try:\n",
    "                # get url content\n",
    "                response = requests.get(pageLink,headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36', })\n",
    "                # get the html\n",
    "                html=response.content\n",
    "                # if we successuflly got the file, break the loop\n",
    "                break \n",
    "            # requests.get() threw an exception, i.e., the attempt to get the response failed\n",
    "            except:\n",
    "                print ('failed attempt #',i)\n",
    "                # wait 2 secs before trying again\n",
    "                time.sleep(2)\n",
    "\n",
    "        if not html:\n",
    "            # couldnt get the page, ignore\n",
    "            print('could not get page #', p)\n",
    "            continue \n",
    "        \n",
    "        # if we got the page, parse the html file\n",
    "        # first, turn it into a beautiful soup object\n",
    "        soup = BeautifulSoup(html.decode('ascii', 'ignore'),'lxml')\n",
    "        \n",
    "        # then get all the review divs\n",
    "        reviews=soup.findAll('div', {'class':re.compile('review_table_row')})\n",
    "        \n",
    "        # grab the information for each review\n",
    "        print ('Parsing page',p)\n",
    "        \n",
    "        \n",
    "        for review in reviews:\n",
    "            \n",
    "            # initialize critic, rating, source, text, date\n",
    "            critic,rating,source,text,date='NA','NA','NA','NA','NA'\n",
    "            \n",
    "            # 1. if there is critic name information, get it\n",
    "            criticChunk=review.find('a',{'href':re.compile('/critic/')})\n",
    "            if criticChunk: \n",
    "                critic=criticChunk.text.strip()\n",
    "\n",
    "            # 2. if there is rating information, get it\n",
    "            '''\n",
    "            fill this in similarly to # 1\n",
    "            '''             \n",
    "                \n",
    "            # 3. if there is source information, get it\n",
    "            '''\n",
    "            fill this in similarly to # 1\n",
    "            '''  \n",
    "            \n",
    "            # 4. if there is text information, get it    \n",
    "            '''\n",
    "            fill this in similarly to # 1\n",
    "            '''  \n",
    "            \n",
    "            # 5. if there is date information, get it    \n",
    "            '''\n",
    "            fill this in similarly to # 1\n",
    "            '''  \n",
    "            \n",
    "            #write everything to file    \t\t\n",
    "            fw.write(critic+'\\t'+rating+'\\t'+source+'\\t'+text+'\\t'+date+'\\n')\n",
    "\n",
    "print ('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
